{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from env import Env\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 这里我们定义多个游戏地图，后面可以使用不同的游戏地图观察agent的行为\n",
    "def game_map_1(environment):\n",
    "    environment.reset()\n",
    "    environment.add_item('yellow_star', (3, 3), credit=100, pickable=True)\n",
    "    environment.add_item('yellow_star', (0, 7), credit=100, pickable=True)\n",
    "    environment.add_item('red_ball', (5, 6), terminal=True, label=\"Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set the environment\n",
    "env = Env((8, 8), (130, 90), default_rewards=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select a game\n",
    "game_map_1(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "0 (0, 0) False\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    reward, next, end = env.step(action)\n",
    "    print(reward, next, end)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下面两个函数将位置信息转换为状态信息\n",
    "def location_one_hot(location, map_dimension):\n",
    "    row, column = location\n",
    "    total_rows, total_columns = map_dimension\n",
    "    \n",
    "    assert row < total_rows and column < total_columns\n",
    "    \n",
    "    # 将`行`和`列`合并为一个ID，后面用于`one hot`编码\n",
    "    location_id = row * total_columns + column\n",
    "    \n",
    "    # `one hot`编码\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    one_hot[location_id] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def location_multi_hot(locations, map_dimension):\n",
    "    total_rows, total_columns = map_dimension\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    \n",
    "    for loc in locations:\n",
    "        row, column = loc\n",
    "        \n",
    "        assert row < total_rows and column < total_columns\n",
    "        \n",
    "        # 将`行`和`列`合并为一个ID用于`one hot`编码\n",
    "        location_id = row * total_columns + column\n",
    "        one_hot[location_id] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "# 下面函数将环境的全部信息转换成状态信息\n",
    "def state_from_environment(environment):\n",
    "    # 环境地图大小\n",
    "    dimension = (environment.map.n_rows, environment.map.n_columns)\n",
    "\n",
    "    # agent状态信息\n",
    "    agent_state = location_one_hot(environment.agent.at, dimension)\n",
    "\n",
    "    star_locations = []\n",
    "    exit_location = None\n",
    "    for item in environment.map.all_items:        \n",
    "        if item.pickable == True:\n",
    "            star_locations.append(item.index)\n",
    "        elif item.terminal == True:\n",
    "            exit_location = item.index\n",
    "        else:\n",
    "            assert False, \"Unknown item in the environment\"\n",
    "            \n",
    "    # 必须给agent设置一个出口\n",
    "    assert exit_location != None, \"You must have a exit point for agent!\"\n",
    "\n",
    "    # 出口Exit状态信息\n",
    "    exit_state = location_one_hot(exit_location, dimension)\n",
    "\n",
    "    # 环境中星星的状态信息\n",
    "    stars_state = location_multi_hot(star_locations, dimension)\n",
    "\n",
    "    # 返回所有信息的组合\n",
    "    return agent_state + exit_state + stars_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "training_episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn = DQN(env.map.n_squares*3, env.action_space.n_actions, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes 200: current loss is 5.5052, average loss is 21.8122\n",
      "trained episodes 400: current loss is 11.5739, average loss is 19.1100\n",
      "trained episodes 600: current loss is 4.7787, average loss is 18.9909\n",
      "trained episodes 800: current loss is 1.6400, average loss is 17.9131\n",
      "trained episodes 1000: current loss is 5.8743, average loss is 17.6030\n",
      "trained episodes 1200: current loss is 3.5462, average loss is 16.9091\n",
      "trained episodes 1400: current loss is 11.3917, average loss is 16.5325\n",
      "trained episodes 1600: current loss is 1.2886, average loss is 15.8282\n",
      "trained episodes 1800: current loss is 12.9774, average loss is 15.6585\n",
      "trained episodes 2000: current loss is 3.0578, average loss is 15.1230\n",
      "trained episodes 2200: current loss is 2.3314, average loss is 14.7498\n",
      "trained episodes 2400: current loss is 3.2766, average loss is 14.5229\n",
      "trained episodes 2600: current loss is 5.5554, average loss is 14.1429\n",
      "trained episodes 2800: current loss is 2.7633, average loss is 13.8948\n",
      "trained episodes 3000: current loss is 82.7041, average loss is 13.5522\n",
      "trained episodes 3200: current loss is 1.8993, average loss is 13.2196\n",
      "trained episodes 3400: current loss is 4.1464, average loss is 12.8114\n",
      "trained episodes 3600: current loss is 0.7090, average loss is 12.6390\n",
      "trained episodes 3800: current loss is 0.3401, average loss is 12.3074\n",
      "trained episodes 4000: current loss is 0.9863, average loss is 12.0282\n"
     ]
    }
   ],
   "source": [
    "training_episodes = 5000\n",
    "total_losses = 0\n",
    "for episode in range(1, training_episodes+1):\n",
    "    env.reset()  # 复位环境\n",
    "    env.show = False\n",
    "    \n",
    "    # 此时环境刚复位，获取此时的环境状态信息\n",
    "    state = state_from_environment(env)\n",
    "\n",
    "    # 调式信息\n",
    "    location = env.agent.at\n",
    "\n",
    "    # 记录此回合agent的经历\n",
    "    this_episode = []\n",
    "    \n",
    "    end = False  # 表明此回合是否结束\n",
    "    while end == False:\n",
    "        # 查询DQN由当前状态获取动作\n",
    "        # 注意：使用DQN时一般将动作编码为从0开始的连续数字，DQN内部以及其输入输出\n",
    "        # 都使用这种数字代表动作。\n",
    "        # 环境理解的动作可能不是数字，所以要进行转换。\n",
    "        action_id = dqn.next_action(state)\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "        # 指导agent走一步，环境返回这一步行动产生的reward，agent的位置和agent是否到达了出口\n",
    "        reward, next_location, end = env.step(action)\n",
    "\n",
    "        # 获取agent走了一步后的环境状态信息\n",
    "        next_state = state_from_environment(env)\n",
    "\n",
    "        # 记录这一步\n",
    "        this_episode.append((state, action_id, reward, next_state))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # 调式信息\n",
    "        #print(\"action {}: {} ----> {} reward {}\".format(action, location, next_location, reward))\n",
    "        #print(next_state)\n",
    "        location = next_location\n",
    "\n",
    "    # 训练DQN\n",
    "    loss = dqn.train_an_episode(this_episode)\n",
    "    total_losses += loss\n",
    "    if episode != 0 and episode % 200 == 0:\n",
    "        print(\"trained episodes {}: current loss is {:.4f}, average loss is {:.4f}\".format(episode, loss, total_losses/episode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
