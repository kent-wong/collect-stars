{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from env import Env\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我们定义多个游戏地图，后面可以使用不同的游戏地图观察agent的行为\n",
    "def game_map_1(environment):\n",
    "    environment.reset()\n",
    "    environment.add_item('yellow_star', (3, 3), credit=100, pickable=True)\n",
    "    environment.add_item('yellow_star', (0, 7), credit=100, pickable=True)\n",
    "    environment.add_item('red_ball', (5, 6), terminal=True, label=\"Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment\n",
    "env = Env((8, 8), (130, 90), default_rewards=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select a game\n",
    "game_map_1(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "0 (0, 1) False\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    reward, next, end = env.step(action)\n",
    "    print(reward, next, end)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下面两个函数将位置信息转换为状态信息\n",
    "def location_one_hot(location, map_dimension):\n",
    "    row, column = location\n",
    "    total_rows, total_columns = map_dimension\n",
    "    \n",
    "    assert row < total_rows and column < total_columns\n",
    "    \n",
    "    # 将`行`和`列`合并为一个ID，后面用于`one hot`编码\n",
    "    location_id = row * total_columns + column\n",
    "    \n",
    "    # `one hot`编码\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    one_hot[location_id] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def location_multi_hot(locations, map_dimension):\n",
    "    total_rows, total_columns = map_dimension\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    \n",
    "    for loc in locations:\n",
    "        row, column = loc\n",
    "        \n",
    "        assert row < total_rows and column < total_columns\n",
    "        \n",
    "        # 将`行`和`列`合并为一个ID用于`one hot`编码\n",
    "        location_id = row * total_columns + column\n",
    "        one_hot[location_id] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "# 下面函数将环境的全部信息转换成状态信息\n",
    "def state_from_environment(environment):\n",
    "    # 环境地图大小\n",
    "    dimension = (environment.map.n_rows, environment.map.n_columns)\n",
    "\n",
    "    # agent状态信息\n",
    "    agent_state = location_one_hot(environment.agent.at, dimension)\n",
    "\n",
    "    star_locations = []\n",
    "    exit_location = None\n",
    "    for item in environment.map.all_items:        \n",
    "        if item.pickable == True:\n",
    "            star_locations.append(item.index)\n",
    "        elif item.terminal == True:\n",
    "            exit_location = item.index\n",
    "        else:\n",
    "            assert False, \"Unknown item in the environment\"\n",
    "            \n",
    "    # 必须给agent设置一个出口\n",
    "    assert exit_location != None, \"You must have a exit point for agent!\"\n",
    "\n",
    "    # 出口Exit状态信息\n",
    "    exit_state = location_one_hot(exit_location, dimension)\n",
    "\n",
    "    # 环境中星星的状态信息\n",
    "    stars_state = location_multi_hot(star_locations, dimension)\n",
    "\n",
    "    # 返回所有信息的组合\n",
    "    return agent_state + exit_state + stars_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "training_episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env.map.n_squares*3, env.action_space.n_actions, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 随机产生经验数据用于后面进行训练\n",
    "\n",
    "while True:\n",
    "    env.reset()\n",
    "    env.show = False\n",
    "    \n",
    "    this_episode = []\n",
    "    state = state_from_environment(env)\n",
    "    end = False\n",
    "    while end == False:\n",
    "        action = env.action_space.sample()\n",
    "        reward, next_location, end = env.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes 200: current loss is 377.2625, average loss is 149.1892\n",
      "trained episodes 400: current loss is 222.8826, average loss is 150.8623\n",
      "trained episodes 600: current loss is 31.8806, average loss is 129.5653\n",
      "trained episodes 800: current loss is 9.6557, average loss is 113.5371\n",
      "trained episodes 1000: current loss is 48.7760, average loss is 103.3547\n"
     ]
    }
   ],
   "source": [
    "training_episodes = 1000\n",
    "total_losses = 0\n",
    "for episode in range(1, training_episodes+1):\n",
    "    env.reset()  # 复位环境\n",
    "    env.show = False\n",
    "    \n",
    "    # 此时环境刚复位，获取此时的环境状态信息\n",
    "    state = state_from_environment(env)\n",
    "\n",
    "    # 调式信息\n",
    "    location = env.agent.at\n",
    "\n",
    "    # 记录此回合agent的经历\n",
    "    this_episode = []\n",
    "    \n",
    "    end = False  # 表明此回合是否结束\n",
    "    while end == False:\n",
    "        # 查询DQN由当前状态获取动作\n",
    "        # 注意：使用DQN时一般将动作编码为从0开始的连续数字，DQN内部以及其输入输出\n",
    "        # 都使用这种数字代表动作。\n",
    "        # 环境理解的动作可能不是数字，所以要进行转换。\n",
    "        action_id = dqn.next_action(state)\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "        # 指导agent走一步，环境返回这一步行动产生的reward，agent的位置和agent是否到达了出口\n",
    "        reward, next_location, end = env.step(action)\n",
    "\n",
    "        # 获取agent走了一步后的环境状态信息\n",
    "        next_state = state_from_environment(env)\n",
    "\n",
    "        # 记录这一步\n",
    "        this_episode.append((state, action_id, reward, next_state))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # 调式信息\n",
    "        #print(\"action {}: {} ----> {} reward {}\".format(action, location, next_location, reward))\n",
    "        #print(next_state)\n",
    "        location = next_location\n",
    "\n",
    "    # 训练DQN\n",
    "    loss = dqn.train_an_episode(this_episode)\n",
    "    total_losses += loss\n",
    "    if episode != 0 and episode % 200 == 0:\n",
    "        print(\"trained episodes {}: current loss is {:.4f}, average loss is {:.4f}\".format(episode, loss, total_losses/episode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
