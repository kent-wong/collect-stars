{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from env import Env\n",
    "from dqn import DQN\n",
    "from chessboard import Chessboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 这里我们定义多个游戏地图，后面可以使用不同的游戏地图观察agent的行为\n",
    "def game_map_1(environment):\n",
    "    environment.reset()\n",
    "    environment.add_item('yellow_star', (3, 3), credit=1000, pickable=True, label=1)\n",
    "    environment.add_item('yellow_star', (0, 7), credit=1000, pickable=True, label=2)\n",
    "    environment.add_item('red_ball', (5, 6), terminal=True, label=\"Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set the environment\n",
    "env = Env((8, 8), (160, 90), default_rewards=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select a game\n",
    "game_map_1(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    reward, next, end, _ = env.step(action)\n",
    "    print(reward, next, end)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compose_state(agent_loc, items_picked, map_size, tunnel=0):\n",
    "    # agent状态信息\n",
    "    agent_state = location_one_hot(agent_loc, map_size)\n",
    "    \n",
    "    # agent已经收集的星星\n",
    "    items_state = location_multi_hot(items_picked, map_size)\n",
    "\n",
    "    return agent_state + items_state + [tunnel]\n",
    "\n",
    "def centralize_range(start, stop=0):\n",
    "    if stop == 0:\n",
    "        stop = start\n",
    "        start = 0\n",
    "        \n",
    "    assert stop >= start\n",
    "    \n",
    "    len = stop - start + 1\n",
    "    middle = (len-1) / 2\n",
    "    start = - middle\n",
    "    stop = middle\n",
    "    \n",
    "    return start, stop\n",
    "\n",
    "def regularize_range(start, stop=0):\n",
    "    begin, end = centralize_range(start, stop)\n",
    "    half = (end-begin) / 2\n",
    "    result = [i/half for i in np.arange(begin, end+1)]\n",
    "    return result\n",
    "\n",
    "# test\n",
    "arg = (3, 6)\n",
    "start, stop = centralize_range(*arg)\n",
    "print(start, stop)\n",
    "numbers = regularize_range(*arg)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regularize_location(loc, map_size):\n",
    "    r, c = loc\n",
    "    height, width = map_size\n",
    "    \n",
    "    rows = regularize_range(height-1)\n",
    "    #print(rows)\n",
    "    \n",
    "    cols = regularize_range(width-1)\n",
    "    #print(cols)\n",
    "    \n",
    "    return rows[r], cols[c]\n",
    "    \n",
    "r, c = regularize_location((0, 1), (8, 8))\n",
    "print(\"location:\", r, c)\n",
    "    \n",
    "def compose_state2(agent_loc, encode, map_size):\n",
    "    agent_row, agent_col = regularize_location(agent_loc, map_size)\n",
    "    return [agent_row, agent_col, encode]\n",
    "\n",
    "state = compose_state2((0, 0), 0, (8, 8))\n",
    "print(state)\n",
    "state = compose_state2((7, 7), 1, (8, 8))\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 下面两个函数将位置信息转换为状态信息\n",
    "def location_one_hot(location, map_dimension):\n",
    "    row, column = location\n",
    "    total_rows, total_columns = map_dimension\n",
    "    \n",
    "    assert row < total_rows and column < total_columns\n",
    "    \n",
    "    # 将`行`和`列`合并为一个ID，后面用于`one hot`编码\n",
    "    location_id = row * total_columns + column\n",
    "    \n",
    "    # `one hot`编码\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    one_hot[location_id] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def location_multi_hot(locations, map_dimension):\n",
    "    total_rows, total_columns = map_dimension\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    \n",
    "    for loc in locations:\n",
    "        row, column = loc\n",
    "        \n",
    "        assert row < total_rows and column < total_columns\n",
    "        \n",
    "        # 将`行`和`列`合并为一个ID用于`one hot`编码\n",
    "        location_id = row * total_columns + column\n",
    "        one_hot[location_id] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "# 下面函数将环境的全部信息转换成状态信息\n",
    "def state_from_environment_old(environment):    \n",
    "    # 环境地图大小\n",
    "    dimension = (environment.map.n_rows, environment.map.n_columns)\n",
    "\n",
    "    # agent状态信息\n",
    "    agent_state = location_one_hot(environment.agent.at, dimension)\n",
    "\n",
    "    star_locations = []\n",
    "    exit_location = None\n",
    "    for item in environment.map.all_items:        \n",
    "        if item.pickable == True:\n",
    "            star_locations.append(item.index)\n",
    "        elif item.terminal == True:\n",
    "            exit_location = item.index\n",
    "        else:\n",
    "            assert False, \"Unknown item in the environment\"\n",
    "            \n",
    "    # 必须给agent设置一个出口\n",
    "    assert exit_location != None, \"You must have a exit point for agent!\"\n",
    "\n",
    "    # 出口Exit状态信息\n",
    "    exit_state = location_one_hot(exit_location, dimension)\n",
    "\n",
    "    # 环境中星星的状态信息\n",
    "    stars_state = location_multi_hot(star_locations, dimension)\n",
    "\n",
    "    n_items = len(environment.agent.bag_of_objects)\n",
    "    \n",
    "    item_locations = []\n",
    "    for item in environment.agent.bag_of_objects:\n",
    "        item_locations.append(item.index)\n",
    "    items_state = location_multi_hot(item_locations, dimension)\n",
    "    \n",
    "    # 返回所有信息的组合\n",
    "    #return agent_state + exit_state + stars_state\n",
    "    return agent_state + items_state + [0]\n",
    "\n",
    "def state_from_environment(env):    \n",
    "    # 环境地图大小\n",
    "    map_size = (env.map.n_rows, env.map.n_columns)\n",
    "    encode = 0\n",
    "    for item in env.agent.bag_of_objects:\n",
    "        item_id = item.label # 编号从1开始\n",
    "        assert item_id > 0\n",
    "        \n",
    "        encode += pow(2, item_id-1)\n",
    "    \n",
    "    state = compose_state2(env.agent.at, encode, map_size)\n",
    "    return state\n",
    "\n",
    "env.reset()\n",
    "state = state_from_environment(env)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# env.map.n_squares*2+1\n",
    "dqn = DQN(3, env.action_space.n_actions, [64, 32], lr, gamma, experience_limit=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 打印Action-Value信息\n",
    "def show_action_values(env):\n",
    "    location = env.agent.at\n",
    "    state = state_from_environment(env)\n",
    "    \n",
    "    state = np.array(state)\n",
    "    matrix_form = state.reshape((1, *state.shape))\n",
    "    action_values = dqn.action_values(matrix_form)[0]  \n",
    "    \n",
    "    # debug\n",
    "    #print(\"begin of iterating...\")\n",
    "    #print(action_values)\n",
    "    \n",
    "    text_dict = {}    \n",
    "    for action_id, value in enumerate(action_values):\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "        value = np.round(value, 2)\n",
    "        \n",
    "        # debug\n",
    "        #print(action, end=\" \")\n",
    "        \n",
    "        text_dict[action] = str(value)\n",
    "\n",
    "    # debug\n",
    "    #print(\"end of iterating...\")\n",
    "    \n",
    "    env.draw_text(location, text_dict)\n",
    "    \n",
    "def _show_state(state, location):    \n",
    "    state = np.array(state)\n",
    "    matrix_form = state.reshape((1, *state.shape))\n",
    "    action_values = dqn.action_values(matrix_form)[0]  \n",
    "    \n",
    "    text_dict = {}    \n",
    "    for action_id, value in enumerate(action_values):\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "        value = np.round(value, 2)\n",
    "        text_dict[action] = str(value)\n",
    "\n",
    "    env.draw_text(location, text_dict)\n",
    "    \n",
    "def show_all_state(env, picked_items, tunnel=0):\n",
    "    map_size = (env.map.n_rows, env.map.n_columns)\n",
    "    locations = [(row, col) for row in range(map_size[0]) for col in range(map_size[1])]\n",
    "    for loc in locations:\n",
    "        state = compose_state(loc, picked_items, map_size, tunnel=tunnel)\n",
    "        _show_state(state, loc)\n",
    "        \n",
    "def show_all_state2(env, items_encode):\n",
    "    map_size = (env.map.n_rows, env.map.n_columns)\n",
    "    locations = [(row, col) for row in range(map_size[0]) for col in range(map_size[1])]\n",
    "    for loc in locations:\n",
    "        state = compose_state2(loc, items_encode, map_size)\n",
    "        _show_state(state, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 随机产生经验数据用于后面进行训练\n",
    "def sample_experience():\n",
    "    while True:\n",
    "        env.reset()\n",
    "        env.show = False\n",
    "        location = env.agent.at\n",
    "        next_location = location\n",
    "        # 获取初始状态\n",
    "        state = state_from_environment(env)\n",
    "        end = False\n",
    "        while end == False:\n",
    "            # 随机选取一个动作\n",
    "            action = env.action_space.sample()\n",
    "            action_id = env.action_space.action_id(action)\n",
    "            reward, next_location, end, _ = env.step(action)\n",
    "\n",
    "            # 获取agent走了一步后的环境状态信息\n",
    "            next_state = state_from_environment(env)\n",
    "            dqn.fill_experience((state, action_id, reward, next_state, end))\n",
    "            state = next_state\n",
    "            location = next_location\n",
    "\n",
    "        # 如果经验数据已满则进行预训练\n",
    "        if dqn.experience.is_full:\n",
    "            break\n",
    "        \n",
    "    \n",
    "n_pretrains = 30\n",
    "for pretrain in range(n_pretrains):\n",
    "    dqn.clear_experience()\n",
    "    sample_experience()\n",
    "    for i in range(100):\n",
    "        batch_size = 400\n",
    "        loss = dqn.train_batch_states(batch_size)\n",
    "\n",
    "        print(\"experience sample {} pretrain batch {}: loss is {}\".format(\n",
    "                                                pretrain,\n",
    "                                                i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dqn.clear_experience()\n",
    "training_episodes = 50000\n",
    "total_losses = 0\n",
    "\n",
    "for episode in range(1, training_episodes+1):\n",
    "    env.reset()  # 复位环境\n",
    "    #env.show = True\n",
    "    env.show = False\n",
    "        \n",
    "    # 此时环境刚复位，获取此时的环境状态信息\n",
    "    state = state_from_environment(env)\n",
    "    \n",
    "    # 记录agent走一步前后的两个位置\n",
    "    location = env.agent.at\n",
    "    next_location = location\n",
    "    \n",
    "    this_episode = []\n",
    "    hit_walls = 0\n",
    "    end = False  # 表明此回合是否结束\n",
    "    while end == False:\n",
    "        # 打印当前状态的Action-Value值\n",
    "        #show_action_values(env)\n",
    "        \n",
    "        # 查询DQN由当前状态获取动作\n",
    "        # 注意：使用DQN时一般将动作编码为从0开始的连续数字，DQN内部以及其输入输出\n",
    "        # 都使用这种数字代表动作。\n",
    "        # 环境理解的动作可能不是数字，所以要进行转换。\n",
    "        action_id = dqn.next_action(state, episode)\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "        # 指导agent走一步，环境返回这一步行动产生的reward，agent的新位置和agent是否到达了出口\n",
    "        reward, next_location, end, _ = env.step(action)\n",
    "        #time.sleep(0.05)\n",
    "\n",
    "        # 获取next state，并将这一步的信息记录入经验数据\n",
    "        next_state = state_from_environment(env)\n",
    "        one_step = (state, action_id, reward, next_state, end)\n",
    "        dqn.fill_experience(one_step)\n",
    "        this_episode.append(one_step)\n",
    "        \n",
    "        if location == next_location:\n",
    "            hit_walls += 1\n",
    "        \n",
    "        state = next_state\n",
    "        location = next_location\n",
    "\n",
    "        if env.steps >= 400:\n",
    "            break\n",
    "            \n",
    "        # 调式信息\n",
    "        #print(\"state:\", state)\n",
    "        #print(\"common state:\", common_state(state))\n",
    "        #print(\"step {}: {} ----> {} {} reward {}\\n\".format(env.steps, location, next_location, action, reward))\n",
    "        #print(next_state)\n",
    "            \n",
    "    batch_size = 100\n",
    "    loss = dqn.train_batch_states(batch_size)\n",
    "    total_losses += loss\n",
    "\n",
    "    # 训练一整条episode\n",
    "    for _ in range(50):\n",
    "        dqn.train_an_episode(this_episode)\n",
    "    \n",
    "    #print(\"items remain in map:\", len(env.pickable_items()))\n",
    "    print(\"# {}: batch avg loss is {:.4f}, agent moved {} steps, hit walls {}, reward is {}\".format(\n",
    "                                            episode, \n",
    "                                            loss / batch_size,\n",
    "                                            env.steps,\n",
    "                                            hit_walls,\n",
    "                                            reward))\n",
    "    #env.show = True\n",
    "    #show_all_state2(env, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.clear_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train(batch):\n",
    "    for i in range(batch):    \n",
    "        # 手工合成state和next state\n",
    "        state = compose_state2((5, 5), 0, (8, 8))\n",
    "        next_state = compose_state2((5, 6), 0, (8, 8))\n",
    "        action_id = env.action_space.action_id('E')\n",
    "        dqn.fill_experience((state, action_id, 1000, next_state, True))\n",
    "\n",
    "        state = compose_state2((6, 5), 0, (8, 8))\n",
    "        next_state = compose_state2((5, 5), 0, (8, 8))\n",
    "        action_id = env.action_space.action_id('N')\n",
    "        dqn.fill_experience((state, action_id, 0, next_state, False))\n",
    "\n",
    "        state = compose_state2((6, 6), 0, (8, 8))\n",
    "        next_state = compose_state2((5, 6), 0, (8, 8))\n",
    "        action_id = env.action_space.action_id('N')\n",
    "        dqn.fill_experience((state, action_id, 1000, next_state, True))\n",
    "         \n",
    "        state = compose_state2((6, 5), 0, (8, 8))\n",
    "        next_state = compose_state2((6, 6), 0, (8, 8))\n",
    "        action_id = env.action_space.action_id('E')\n",
    "        dqn.fill_experience((state, action_id, 0, next_state, False))\n",
    "            \n",
    "    dqn.train_batch_states(batch)\n",
    "    env.show = True\n",
    "    show_all_state2(env, 0)\n",
    "    \n",
    "#test_train(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_loop(batch):\n",
    "    for i in range(batch):    \n",
    "        state = compose_state2((0, 1), 0, (8, 8))\n",
    "        next_state = compose_state2((0, 1), 0, (8, 8))\n",
    "        action_id = env.action_space.action_id('N')\n",
    "        dqn.fill_experience((state, action_id, 0, next_state, True))\n",
    "\n",
    "    dqn.train_batch_states(batch)\n",
    "    env.show = True\n",
    "    show_all_state2(env, 0)\n",
    "    \n",
    "#test_train_loop(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = state_from_environment(env)\n",
    "print(state)\n",
    "env.show = True\n",
    "show_all_state2(env, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ---- 测试 ----\n",
    "# 复位环境并获取初始状态\n",
    "env.reset()\n",
    "env.show = True\n",
    "\n",
    "end = False  # 表明此回合是否结束\n",
    "while end == False:\n",
    "    # debug\n",
    "    show_action_values(env)\n",
    "\n",
    "    # 获取环境状态\n",
    "    state = state_from_environment(env)\n",
    "    \n",
    "    # 从DQN获取Policy并选取具有最大Value值的动作作为下一个动作\n",
    "    action_id = dqn.best_action(state)\n",
    "    action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "    # debug\n",
    "    #print(\"action:\", action)\n",
    "    \n",
    "    # agent执行此动作\n",
    "    reward, next_location, end, _ = env.step(action)\n",
    "    time.sleep(0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
