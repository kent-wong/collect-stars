{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from env import Env\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 这里我们定义多个游戏地图，后面可以使用不同的游戏地图观察agent的行为\n",
    "def game_map_1(environment):\n",
    "    environment.reset()\n",
    "    environment.add_item('yellow_star', (3, 3), credit=1000, pickable=True)\n",
    "    environment.add_item('yellow_star', (0, 7), credit=1000, pickable=True)\n",
    "    environment.add_item('red_ball', (5, 6), terminal=True, label=\"Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set the environment\n",
    "env = Env((8, 8), (130, 90), default_rewards=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select a game\n",
    "game_map_1(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "0 (0, 0) False\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    reward, next, end = env.step(action)\n",
    "    print(reward, next, end)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 下面两个函数将位置信息转换为状态信息\n",
    "def location_one_hot(location, map_dimension):\n",
    "    row, column = location\n",
    "    total_rows, total_columns = map_dimension\n",
    "    \n",
    "    assert row < total_rows and column < total_columns\n",
    "    \n",
    "    # 将`行`和`列`合并为一个ID，后面用于`one hot`编码\n",
    "    location_id = row * total_columns + column\n",
    "    \n",
    "    # `one hot`编码\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    one_hot[location_id] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def location_multi_hot(locations, map_dimension):\n",
    "    total_rows, total_columns = map_dimension\n",
    "    one_hot = [0] * (total_rows * total_columns)\n",
    "    \n",
    "    for loc in locations:\n",
    "        row, column = loc\n",
    "        \n",
    "        assert row < total_rows and column < total_columns\n",
    "        \n",
    "        # 将`行`和`列`合并为一个ID用于`one hot`编码\n",
    "        location_id = row * total_columns + column\n",
    "        one_hot[location_id] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "# 下面函数将环境的全部信息转换成状态信息\n",
    "def state_from_environment(environment):    \n",
    "    # 环境地图大小\n",
    "    dimension = (environment.map.n_rows, environment.map.n_columns)\n",
    "\n",
    "    # agent状态信息\n",
    "    agent_state = location_one_hot(environment.agent.at, dimension)\n",
    "\n",
    "    star_locations = []\n",
    "    exit_location = None\n",
    "    for item in environment.map.all_items:        \n",
    "        if item.pickable == True:\n",
    "            star_locations.append(item.index)\n",
    "        elif item.terminal == True:\n",
    "            exit_location = item.index\n",
    "        else:\n",
    "            assert False, \"Unknown item in the environment\"\n",
    "            \n",
    "    # 必须给agent设置一个出口\n",
    "    assert exit_location != None, \"You must have a exit point for agent!\"\n",
    "\n",
    "    # 出口Exit状态信息\n",
    "    exit_state = location_one_hot(exit_location, dimension)\n",
    "\n",
    "    # 环境中星星的状态信息\n",
    "    stars_state = location_multi_hot(star_locations, dimension)\n",
    "\n",
    "    n_items = len(environment.agent.bag_of_objects)\n",
    "\n",
    "    # 返回所有信息的组合\n",
    "    return agent_state + exit_state + stars_state\n",
    "\n",
    "def fake_state(env):\n",
    "    return [0] * (env.map.n_squares*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "\n",
    "training_episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dqn = DQN(env.map.n_squares*3, env.action_space.n_actions, lr, gamma, experience_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': 0, 'E': 1, 'N': 3, 'W': 2}\n"
     ]
    }
   ],
   "source": [
    "# 调试信息\n",
    "action_dict = env.action_space.dict_from_actions()\n",
    "print(action_dict)\n",
    "episode = 0\n",
    "\n",
    "# 随机产生经验数据用于后面进行训练\n",
    "while True:\n",
    "    env.reset()\n",
    "    env.show = False\n",
    "    \n",
    "    location = env.agent.at\n",
    "    next_location = location\n",
    "    \n",
    "    # 获取初始状态\n",
    "    state = state_from_environment(env)\n",
    "\n",
    "    this_episode = []\n",
    "    episode += 1\n",
    "    \n",
    "    end = False\n",
    "    while end == False:\n",
    "        # 随机选取一个动作\n",
    "        action = env.action_space.sample()\n",
    "        reward, next_location, end = env.step(action)\n",
    "\n",
    "        if location == next_location:\n",
    "            next_state = fake_state(env)\n",
    "            end = True\n",
    "        else:\n",
    "            # 获取agent走了一步后的环境状态信息\n",
    "            next_state = state_from_environment(env)\n",
    "\n",
    "        # 记录这一步\n",
    "        action_id = env.action_space.action_id(action)\n",
    "        this_episode.append((state, action_id, reward, next_state, end))\n",
    "\n",
    "        # 调试信息\n",
    "        #print(\"action: {}, action_id {}\".format(action, action_id))\n",
    "        \n",
    "        state = next_state\n",
    "        location = next_location\n",
    "        \n",
    "    # 调试信息\n",
    "    #print(\"current episode {}\".format(episode))\n",
    "    \n",
    "    # 将当前回合加入经验数据\n",
    "    dqn.fill_experience(this_episode)\n",
    "    \n",
    "    # 如果经验数据已满则跳出循环\n",
    "    if dqn.experience.is_full:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 打印Action-Value信息\n",
    "def show_action_values(env):\n",
    "    location = env.agent.at\n",
    "    state = state_from_environment(env)\n",
    "    \n",
    "    state = np.array(state)\n",
    "    matrix_form = state.reshape((1, *state.shape))\n",
    "    action_values = dqn.action_values(matrix_form)[0]  \n",
    "    \n",
    "    # debug\n",
    "    #print(\"begin of iterating...\")\n",
    "    #print(action_values)\n",
    "    \n",
    "    text_dict = {}    \n",
    "    for action_id, value in enumerate(action_values):\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "        value = np.round(value, 2)\n",
    "        \n",
    "        # debug\n",
    "        #print(action, end=\" \")\n",
    "        \n",
    "        text_dict[action] = str(value)\n",
    "\n",
    "    # debug\n",
    "    #print(\"end of iterating...\")\n",
    "    \n",
    "    env.draw_text(location, text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes 1: current batch avg loss is 0.0030, total avg loss is 0.0030\n",
      "trained episodes 21: current batch avg loss is 0.0028, total avg loss is 0.0003\n",
      "trained episodes 41: current batch avg loss is 0.0028, total avg loss is 0.0002\n",
      "trained episodes 61: current batch avg loss is 0.0028, total avg loss is 0.0002\n",
      "trained episodes 81: current batch avg loss is 0.0026, total avg loss is 0.0002\n",
      "trained episodes 101: current batch avg loss is 0.0024, total avg loss is 0.0002\n",
      "trained episodes 121: current batch avg loss is 0.0024, total avg loss is 0.0002\n",
      "trained episodes 141: current batch avg loss is 0.0022, total avg loss is 0.0001\n",
      "trained episodes 161: current batch avg loss is 0.0021, total avg loss is 0.0001\n",
      "trained episodes 181: current batch avg loss is 0.0019, total avg loss is 0.0001\n",
      "trained episodes 201: current batch avg loss is 0.0017, total avg loss is 0.0001\n",
      "trained episodes 221: current batch avg loss is 0.0016, total avg loss is 0.0001\n",
      "trained episodes 241: current batch avg loss is 0.0014, total avg loss is 0.0001\n",
      "trained episodes 261: current batch avg loss is 0.0013, total avg loss is 0.0001\n",
      "trained episodes 281: current batch avg loss is 0.0012, total avg loss is 0.0001\n",
      "trained episodes 301: current batch avg loss is 0.0011, total avg loss is 0.0001\n",
      "trained episodes 321: current batch avg loss is 0.0009, total avg loss is 0.0001\n",
      "trained episodes 341: current batch avg loss is 0.0005, total avg loss is 0.0001\n",
      "trained episodes 361: current batch avg loss is 0.0005, total avg loss is 0.0001\n",
      "trained episodes 381: current batch avg loss is 0.0004, total avg loss is 0.0001\n",
      "trained episodes 401: current batch avg loss is 0.0006, total avg loss is 0.0001\n",
      "trained episodes 421: current batch avg loss is 0.0006, total avg loss is 0.0001\n",
      "trained episodes 441: current batch avg loss is 0.0005, total avg loss is 0.0001\n",
      "trained episodes 461: current batch avg loss is 0.0004, total avg loss is 0.0001\n",
      "trained episodes 481: current batch avg loss is 0.0011, total avg loss is 0.0001\n",
      "trained episodes 501: current batch avg loss is 0.0004, total avg loss is 0.0001\n",
      "trained episodes 521: current batch avg loss is 0.0008, total avg loss is 0.0001\n",
      "trained episodes 541: current batch avg loss is 0.0008, total avg loss is 0.0001\n",
      "trained episodes 561: current batch avg loss is 0.0008, total avg loss is 0.0001\n",
      "trained episodes 581: current batch avg loss is 0.0006, total avg loss is 0.0001\n",
      "trained episodes 601: current batch avg loss is 0.0006, total avg loss is 0.0001\n",
      "trained episodes 621: current batch avg loss is 0.0005, total avg loss is 0.0001\n",
      "trained episodes 641: current batch avg loss is 0.0003, total avg loss is 0.0001\n",
      "trained episodes 661: current batch avg loss is 0.0002, total avg loss is 0.0001\n",
      "trained episodes 681: current batch avg loss is 0.0002, total avg loss is 0.0001\n",
      "trained episodes 701: current batch avg loss is 0.0003, total avg loss is 0.0001\n",
      "trained episodes 721: current batch avg loss is 87.6488, total avg loss is 0.1216\n",
      "trained episodes 741: current batch avg loss is 0.0002, total avg loss is 0.1183\n",
      "trained episodes 761: current batch avg loss is 0.0002, total avg loss is 0.1152\n",
      "trained episodes 781: current batch avg loss is 0.0002, total avg loss is 0.1123\n",
      "trained episodes 801: current batch avg loss is 0.0004, total avg loss is 0.1095\n",
      "trained episodes 821: current batch avg loss is 0.0005, total avg loss is 0.1068\n",
      "trained episodes 841: current batch avg loss is 0.0004, total avg loss is 0.1043\n",
      "trained episodes 861: current batch avg loss is 0.0004, total avg loss is 0.1019\n",
      "trained episodes 881: current batch avg loss is 0.0005, total avg loss is 0.0995\n",
      "trained episodes 901: current batch avg loss is 0.0004, total avg loss is 0.0973\n",
      "trained episodes 921: current batch avg loss is 0.0004, total avg loss is 0.0952\n",
      "trained episodes 941: current batch avg loss is 0.0005, total avg loss is 0.0932\n",
      "trained episodes 961: current batch avg loss is 0.0005, total avg loss is 0.0913\n",
      "trained episodes 981: current batch avg loss is 0.0005, total avg loss is 0.0894\n",
      "trained episodes 1001: current batch avg loss is 0.0004, total avg loss is 0.0876\n",
      "trained episodes 1021: current batch avg loss is 0.0004, total avg loss is 0.0859\n",
      "trained episodes 1041: current batch avg loss is 0.0004, total avg loss is 0.0842\n",
      "trained episodes 1061: current batch avg loss is 0.0004, total avg loss is 0.0827\n",
      "trained episodes 1081: current batch avg loss is 0.0004, total avg loss is 0.0811\n",
      "trained episodes 1101: current batch avg loss is 0.0004, total avg loss is 0.0797\n",
      "trained episodes 1121: current batch avg loss is 0.0004, total avg loss is 0.0782\n",
      "trained episodes 1141: current batch avg loss is 0.0003, total avg loss is 0.0769\n",
      "trained episodes 1161: current batch avg loss is 0.0003, total avg loss is 0.0755\n",
      "trained episodes 1181: current batch avg loss is 0.0003, total avg loss is 0.0743\n",
      "trained episodes 1201: current batch avg loss is 0.0003, total avg loss is 0.0730\n",
      "trained episodes 1221: current batch avg loss is 0.0003, total avg loss is 0.0718\n",
      "trained episodes 1241: current batch avg loss is 0.0002, total avg loss is 0.0707\n",
      "trained episodes 1261: current batch avg loss is 0.0002, total avg loss is 0.0696\n",
      "trained episodes 1281: current batch avg loss is 0.0002, total avg loss is 0.0685\n",
      "trained episodes 1301: current batch avg loss is 0.0001, total avg loss is 0.0674\n",
      "trained episodes 1321: current batch avg loss is 0.0001, total avg loss is 0.0664\n",
      "trained episodes 1341: current batch avg loss is 0.0002, total avg loss is 0.0654\n",
      "trained episodes 1361: current batch avg loss is 0.0001, total avg loss is 0.0644\n",
      "trained episodes 1381: current batch avg loss is 0.0001, total avg loss is 0.0635\n",
      "trained episodes 1401: current batch avg loss is 0.0001, total avg loss is 0.0626\n",
      "trained episodes 1421: current batch avg loss is 0.0001, total avg loss is 0.0617\n",
      "trained episodes 1441: current batch avg loss is 0.0001, total avg loss is 0.0609\n",
      "trained episodes 1461: current batch avg loss is 0.0001, total avg loss is 0.0600\n",
      "trained episodes 1481: current batch avg loss is 0.0001, total avg loss is 0.0592\n",
      "trained episodes 1501: current batch avg loss is 0.0001, total avg loss is 0.0584\n",
      "trained episodes 1521: current batch avg loss is 0.0001, total avg loss is 0.0577\n",
      "trained episodes 1541: current batch avg loss is 0.0001, total avg loss is 0.0569\n",
      "trained episodes 1561: current batch avg loss is 0.0001, total avg loss is 0.0562\n",
      "trained episodes 1581: current batch avg loss is 0.0001, total avg loss is 0.0555\n",
      "trained episodes 1601: current batch avg loss is 0.0001, total avg loss is 0.0548\n",
      "trained episodes 1621: current batch avg loss is 0.0001, total avg loss is 0.0541\n",
      "trained episodes 1641: current batch avg loss is 0.0001, total avg loss is 0.0534\n",
      "trained episodes 1661: current batch avg loss is 0.0001, total avg loss is 0.0528\n",
      "trained episodes 1681: current batch avg loss is 0.0001, total avg loss is 0.0522\n",
      "trained episodes 1701: current batch avg loss is 0.0001, total avg loss is 0.0516\n",
      "trained episodes 1721: current batch avg loss is 0.0001, total avg loss is 0.0510\n",
      "trained episodes 1741: current batch avg loss is 0.0001, total avg loss is 0.0504\n",
      "trained episodes 1761: current batch avg loss is 0.0001, total avg loss is 0.0498\n",
      "trained episodes 1781: current batch avg loss is 0.0001, total avg loss is 0.0492\n",
      "trained episodes 1801: current batch avg loss is 0.0001, total avg loss is 0.0487\n",
      "trained episodes 1821: current batch avg loss is 0.0001, total avg loss is 0.0482\n",
      "trained episodes 1841: current batch avg loss is 0.0002, total avg loss is 0.0476\n",
      "trained episodes 1861: current batch avg loss is 0.0002, total avg loss is 0.0471\n",
      "trained episodes 1881: current batch avg loss is 0.0001, total avg loss is 0.0466\n",
      "trained episodes 1901: current batch avg loss is 0.0000, total avg loss is 0.0461\n",
      "trained episodes 1921: current batch avg loss is 0.0000, total avg loss is 0.0457\n",
      "trained episodes 1941: current batch avg loss is 0.0000, total avg loss is 0.0452\n",
      "trained episodes 1961: current batch avg loss is 0.0000, total avg loss is 0.0447\n",
      "trained episodes 1981: current batch avg loss is 0.0000, total avg loss is 0.0443\n",
      "trained episodes 2001: current batch avg loss is 0.0000, total avg loss is 0.0438\n",
      "trained episodes 2021: current batch avg loss is 0.0000, total avg loss is 0.0434\n",
      "trained episodes 2041: current batch avg loss is 0.0001, total avg loss is 0.0430\n",
      "trained episodes 2061: current batch avg loss is 0.0000, total avg loss is 0.0426\n",
      "trained episodes 2081: current batch avg loss is 0.0000, total avg loss is 0.0421\n",
      "trained episodes 2101: current batch avg loss is 0.0000, total avg loss is 0.0417\n",
      "trained episodes 2121: current batch avg loss is 0.0001, total avg loss is 0.0414\n",
      "trained episodes 2141: current batch avg loss is 0.0001, total avg loss is 0.0410\n",
      "trained episodes 2161: current batch avg loss is 0.0001, total avg loss is 0.0406\n",
      "trained episodes 2181: current batch avg loss is 0.0001, total avg loss is 0.0402\n",
      "trained episodes 2201: current batch avg loss is 0.0001, total avg loss is 0.0398\n",
      "trained episodes 2221: current batch avg loss is 0.0001, total avg loss is 0.0395\n",
      "trained episodes 2241: current batch avg loss is 0.0001, total avg loss is 0.0391\n",
      "trained episodes 2261: current batch avg loss is 0.0001, total avg loss is 0.0388\n",
      "trained episodes 2281: current batch avg loss is 0.0001, total avg loss is 0.0385\n",
      "trained episodes 2301: current batch avg loss is 0.0000, total avg loss is 0.0381\n",
      "trained episodes 2321: current batch avg loss is 0.0001, total avg loss is 0.0378\n",
      "trained episodes 2341: current batch avg loss is 0.0001, total avg loss is 0.0375\n",
      "trained episodes 2361: current batch avg loss is 0.0001, total avg loss is 0.0371\n",
      "trained episodes 2381: current batch avg loss is 0.0001, total avg loss is 0.0368\n",
      "trained episodes 2401: current batch avg loss is 0.0001, total avg loss is 0.0365\n",
      "trained episodes 2421: current batch avg loss is 0.0001, total avg loss is 0.0362\n",
      "trained episodes 2441: current batch avg loss is 0.0000, total avg loss is 0.0359\n",
      "trained episodes 2461: current batch avg loss is 0.0001, total avg loss is 0.0356\n",
      "trained episodes 2481: current batch avg loss is 0.0000, total avg loss is 0.0354\n",
      "trained episodes 2501: current batch avg loss is 0.0000, total avg loss is 0.0351\n",
      "trained episodes 2521: current batch avg loss is 0.0000, total avg loss is 0.0348\n",
      "trained episodes 2541: current batch avg loss is 0.0000, total avg loss is 0.0345\n",
      "trained episodes 2561: current batch avg loss is 0.0000, total avg loss is 0.0342\n",
      "trained episodes 2581: current batch avg loss is 0.0001, total avg loss is 0.0340\n",
      "trained episodes 2601: current batch avg loss is 0.0001, total avg loss is 0.0337\n",
      "trained episodes 2621: current batch avg loss is 0.0000, total avg loss is 0.0335\n",
      "trained episodes 2641: current batch avg loss is 0.0000, total avg loss is 0.0332\n",
      "trained episodes 2661: current batch avg loss is 0.0000, total avg loss is 0.0330\n",
      "trained episodes 2681: current batch avg loss is 0.0000, total avg loss is 0.0327\n",
      "trained episodes 2701: current batch avg loss is 0.0000, total avg loss is 0.0325\n",
      "trained episodes 2721: current batch avg loss is 0.0000, total avg loss is 0.0322\n",
      "trained episodes 2741: current batch avg loss is 0.0000, total avg loss is 0.0320\n",
      "trained episodes 2761: current batch avg loss is 0.0000, total avg loss is 0.0318\n",
      "trained episodes 2781: current batch avg loss is 0.0001, total avg loss is 0.0315\n",
      "trained episodes 2801: current batch avg loss is 0.0000, total avg loss is 0.0313\n",
      "trained episodes 2821: current batch avg loss is 0.0000, total avg loss is 0.0311\n",
      "trained episodes 2841: current batch avg loss is 0.0000, total avg loss is 0.0309\n",
      "trained episodes 2861: current batch avg loss is 0.0000, total avg loss is 0.0307\n",
      "trained episodes 2881: current batch avg loss is 0.0001, total avg loss is 0.0304\n",
      "trained episodes 2901: current batch avg loss is 0.0001, total avg loss is 0.0302\n",
      "trained episodes 2921: current batch avg loss is 0.0001, total avg loss is 0.0300\n",
      "trained episodes 2941: current batch avg loss is 0.0000, total avg loss is 0.0298\n",
      "trained episodes 2961: current batch avg loss is 0.0000, total avg loss is 0.0296\n",
      "trained episodes 2981: current batch avg loss is 0.0000, total avg loss is 0.0294\n",
      "trained episodes 3001: current batch avg loss is 0.0000, total avg loss is 0.0292\n",
      "trained episodes 3021: current batch avg loss is 0.0000, total avg loss is 0.0290\n",
      "trained episodes 3041: current batch avg loss is 0.0001, total avg loss is 0.0288\n",
      "trained episodes 3061: current batch avg loss is 0.0000, total avg loss is 0.0287\n",
      "trained episodes 3081: current batch avg loss is 0.0001, total avg loss is 0.0285\n",
      "trained episodes 3101: current batch avg loss is 0.0000, total avg loss is 0.0283\n",
      "trained episodes 3121: current batch avg loss is 0.0000, total avg loss is 0.0281\n",
      "trained episodes 3141: current batch avg loss is 80.3536, total avg loss is 0.0535\n",
      "trained episodes 3161: current batch avg loss is 0.0003, total avg loss is 0.0532\n",
      "trained episodes 3181: current batch avg loss is 0.0000, total avg loss is 0.0528\n",
      "trained episodes 3201: current batch avg loss is 0.0001, total avg loss is 0.0525\n",
      "trained episodes 3221: current batch avg loss is 0.0001, total avg loss is 0.0522\n",
      "trained episodes 3241: current batch avg loss is 0.0001, total avg loss is 0.0519\n",
      "trained episodes 3261: current batch avg loss is 0.0001, total avg loss is 0.0515\n",
      "trained episodes 3281: current batch avg loss is 0.0001, total avg loss is 0.0512\n",
      "trained episodes 3301: current batch avg loss is 0.0001, total avg loss is 0.0509\n",
      "trained episodes 3321: current batch avg loss is 0.0001, total avg loss is 0.0506\n",
      "trained episodes 3341: current batch avg loss is 0.0002, total avg loss is 0.0503\n",
      "trained episodes 3361: current batch avg loss is 0.0002, total avg loss is 0.0500\n",
      "trained episodes 3381: current batch avg loss is 0.0002, total avg loss is 0.0497\n",
      "trained episodes 3401: current batch avg loss is 0.0002, total avg loss is 0.0494\n",
      "trained episodes 3421: current batch avg loss is 0.0002, total avg loss is 0.0491\n",
      "trained episodes 3441: current batch avg loss is 0.0002, total avg loss is 0.0488\n",
      "trained episodes 3461: current batch avg loss is 0.0002, total avg loss is 0.0486\n",
      "trained episodes 3481: current batch avg loss is 0.0002, total avg loss is 0.0483\n",
      "trained episodes 3501: current batch avg loss is 0.0002, total avg loss is 0.0480\n",
      "trained episodes 3521: current batch avg loss is 0.0002, total avg loss is 0.0477\n",
      "trained episodes 3541: current batch avg loss is 0.0002, total avg loss is 0.0475\n",
      "trained episodes 3561: current batch avg loss is 0.0001, total avg loss is 0.0472\n",
      "trained episodes 3581: current batch avg loss is 0.0001, total avg loss is 0.0469\n",
      "trained episodes 3601: current batch avg loss is 0.0001, total avg loss is 0.0467\n",
      "trained episodes 3621: current batch avg loss is 0.0001, total avg loss is 0.0464\n",
      "trained episodes 3641: current batch avg loss is 0.0001, total avg loss is 0.0462\n",
      "trained episodes 3661: current batch avg loss is 0.0001, total avg loss is 0.0459\n",
      "trained episodes 3681: current batch avg loss is 0.0001, total avg loss is 0.0457\n",
      "trained episodes 3701: current batch avg loss is 0.0001, total avg loss is 0.0454\n",
      "trained episodes 3721: current batch avg loss is 0.0001, total avg loss is 0.0452\n",
      "trained episodes 3741: current batch avg loss is 0.0001, total avg loss is 0.0449\n",
      "trained episodes 3761: current batch avg loss is 0.0001, total avg loss is 0.0447\n",
      "trained episodes 3781: current batch avg loss is 0.0001, total avg loss is 0.0445\n",
      "trained episodes 3801: current batch avg loss is 0.0001, total avg loss is 0.0442\n",
      "trained episodes 3821: current batch avg loss is 0.0001, total avg loss is 0.0440\n",
      "trained episodes 3841: current batch avg loss is 0.0001, total avg loss is 0.0438\n",
      "trained episodes 3861: current batch avg loss is 0.0000, total avg loss is 0.0435\n",
      "trained episodes 3881: current batch avg loss is 0.0000, total avg loss is 0.0433\n",
      "trained episodes 3901: current batch avg loss is 0.0000, total avg loss is 0.0431\n",
      "trained episodes 3921: current batch avg loss is 0.0000, total avg loss is 0.0429\n",
      "trained episodes 3941: current batch avg loss is 0.0000, total avg loss is 0.0426\n",
      "trained episodes 3961: current batch avg loss is 0.0000, total avg loss is 0.0424\n",
      "trained episodes 3981: current batch avg loss is 0.0000, total avg loss is 0.0422\n",
      "trained episodes 4001: current batch avg loss is 0.0000, total avg loss is 0.0420\n",
      "trained episodes 4021: current batch avg loss is 0.0000, total avg loss is 0.0418\n",
      "trained episodes 4041: current batch avg loss is 0.0000, total avg loss is 0.0416\n",
      "trained episodes 4061: current batch avg loss is 0.0000, total avg loss is 0.0414\n",
      "trained episodes 4081: current batch avg loss is 0.0000, total avg loss is 0.0412\n",
      "trained episodes 4101: current batch avg loss is 0.0000, total avg loss is 0.0410\n",
      "trained episodes 4121: current batch avg loss is 0.0000, total avg loss is 0.0408\n",
      "trained episodes 4141: current batch avg loss is 0.0000, total avg loss is 0.0406\n",
      "trained episodes 4161: current batch avg loss is 0.0000, total avg loss is 0.0404\n",
      "trained episodes 4181: current batch avg loss is 0.0001, total avg loss is 0.0402\n",
      "trained episodes 4201: current batch avg loss is 0.0000, total avg loss is 0.0400\n",
      "trained episodes 4221: current batch avg loss is 0.0000, total avg loss is 0.0398\n",
      "trained episodes 4241: current batch avg loss is 0.0000, total avg loss is 0.0396\n",
      "trained episodes 4261: current batch avg loss is 0.0000, total avg loss is 0.0394\n",
      "trained episodes 4281: current batch avg loss is 0.0000, total avg loss is 0.0393\n",
      "trained episodes 4301: current batch avg loss is 0.0000, total avg loss is 0.0391\n",
      "trained episodes 4321: current batch avg loss is 0.0000, total avg loss is 0.0389\n",
      "trained episodes 4341: current batch avg loss is 0.0000, total avg loss is 0.0387\n",
      "trained episodes 4361: current batch avg loss is 0.0000, total avg loss is 0.0385\n",
      "trained episodes 4381: current batch avg loss is 0.0000, total avg loss is 0.0384\n",
      "trained episodes 4401: current batch avg loss is 0.0000, total avg loss is 0.0382\n",
      "trained episodes 4421: current batch avg loss is 0.0000, total avg loss is 0.0380\n",
      "trained episodes 4441: current batch avg loss is 0.0000, total avg loss is 0.0378\n",
      "trained episodes 4461: current batch avg loss is 0.0000, total avg loss is 0.0377\n",
      "trained episodes 4481: current batch avg loss is 0.0000, total avg loss is 0.0375\n",
      "trained episodes 4501: current batch avg loss is 0.0000, total avg loss is 0.0373\n",
      "trained episodes 4521: current batch avg loss is 0.0000, total avg loss is 0.0372\n",
      "trained episodes 4541: current batch avg loss is 0.0000, total avg loss is 0.0370\n",
      "trained episodes 4561: current batch avg loss is 0.0000, total avg loss is 0.0368\n",
      "trained episodes 4581: current batch avg loss is 0.0000, total avg loss is 0.0367\n",
      "trained episodes 4601: current batch avg loss is 0.0000, total avg loss is 0.0365\n",
      "trained episodes 4621: current batch avg loss is 0.0000, total avg loss is 0.0364\n",
      "trained episodes 4641: current batch avg loss is 0.0000, total avg loss is 0.0362\n",
      "trained episodes 4661: current batch avg loss is 0.0000, total avg loss is 0.0361\n",
      "trained episodes 4681: current batch avg loss is 0.0000, total avg loss is 0.0359\n",
      "trained episodes 4701: current batch avg loss is 0.0000, total avg loss is 0.0358\n",
      "trained episodes 4721: current batch avg loss is 0.0000, total avg loss is 0.0356\n",
      "trained episodes 4741: current batch avg loss is 0.0000, total avg loss is 0.0354\n",
      "trained episodes 4761: current batch avg loss is 0.0000, total avg loss is 0.0353\n",
      "trained episodes 4781: current batch avg loss is 0.0000, total avg loss is 0.0352\n",
      "trained episodes 4801: current batch avg loss is 0.0000, total avg loss is 0.0350\n",
      "trained episodes 4821: current batch avg loss is 0.0000, total avg loss is 0.0349\n",
      "trained episodes 4841: current batch avg loss is 0.0000, total avg loss is 0.0347\n",
      "trained episodes 4861: current batch avg loss is 0.0000, total avg loss is 0.0346\n",
      "trained episodes 4881: current batch avg loss is 0.0000, total avg loss is 0.0344\n",
      "trained episodes 4901: current batch avg loss is 0.0000, total avg loss is 0.0343\n",
      "trained episodes 4921: current batch avg loss is 0.0000, total avg loss is 0.0342\n",
      "trained episodes 4941: current batch avg loss is 0.0000, total avg loss is 0.0340\n",
      "trained episodes 4961: current batch avg loss is 0.0000, total avg loss is 0.0339\n",
      "trained episodes 4981: current batch avg loss is 0.0000, total avg loss is 0.0337\n"
     ]
    }
   ],
   "source": [
    "training_episodes = 5000\n",
    "total_losses = 0\n",
    "max_value = 0\n",
    "for episode in range(1, training_episodes+1):\n",
    "    env.reset()  # 复位环境\n",
    "    env.show = False\n",
    "    \n",
    "    # 此时环境刚复位，获取此时的环境状态信息\n",
    "    state = state_from_environment(env)\n",
    "\n",
    "    # 记录agent走一步前后的两个位置\n",
    "    location = env.agent.at\n",
    "    next_location = location\n",
    "    \n",
    "    # 记录此回合agent的经历\n",
    "    this_episode = []\n",
    "    \n",
    "    # 调式信息\n",
    "    not_moving = []\n",
    "    \n",
    "    end = False  # 表明此回合是否结束\n",
    "    while end == False:\n",
    "        # 打印当前状态的Action-Value值\n",
    "        show_action_values(env)\n",
    "        \n",
    "        # 查询DQN由当前状态获取动作\n",
    "        # 注意：使用DQN时一般将动作编码为从0开始的连续数字，DQN内部以及其输入输出\n",
    "        # 都使用这种数字代表动作。\n",
    "        # 环境理解的动作可能不是数字，所以要进行转换。\n",
    "        action_id = dqn.next_action(state)\n",
    "        action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "        # 指导agent走一步，环境返回这一步行动产生的reward，agent的新位置和agent是否到达了出口\n",
    "        reward, next_location, end = env.step(action)\n",
    "        if location == next_location:\n",
    "            next_state = fake_state(env)\n",
    "            end = True\n",
    "        else:\n",
    "            # 获取agent走了一步后的环境状态信息\n",
    "            next_state = state_from_environment(env)\n",
    "\n",
    "        # 记录这一步\n",
    "        this_episode.append((state, action_id, reward, next_state, end))\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        # 调式信息\n",
    "        if location == next_location:\n",
    "            not_moving.append(env.steps)\n",
    "        #print(state)\n",
    "        #print(\"step {}: {} ----> {} {} reward {}\".format(env.steps, location, next_location, action, reward))\n",
    "        #print(next_state)\n",
    "        location = next_location\n",
    "            \n",
    "    \n",
    "    dqn.fill_experience(this_episode)\n",
    "    \n",
    "    # 训练DQN\n",
    "    if dqn.experience.is_full:\n",
    "        batch_size = 20\n",
    "        loss = dqn.learn_from_experience(batch_size)\n",
    "        dqn.experience.clear()\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #loss = dqn.train_an_episode(this_episode)\n",
    "    \n",
    "    total_losses += loss\n",
    "\n",
    "    if batch_size == 0:\n",
    "        print(\"trained episodes {}: current loss is {:.4f}, total avg loss is {:.4f}\".format(\n",
    "                                            episode, \n",
    "                                            loss, \n",
    "                                            total_losses/episode))\n",
    "    else:\n",
    "        print(\"trained episodes {}: current batch avg loss is {:.4f}, total avg loss is {:.4f}\".format(\n",
    "                                            episode, \n",
    "                                            loss / batch_size, \n",
    "                                            total_losses/(episode*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ---- 测试 ----\n",
    "# 复位环境并获取初始状态\n",
    "env.reset()\n",
    "env.show = True\n",
    "\n",
    "end = False  # 表明此回合是否结束\n",
    "while end == False:\n",
    "    # debug\n",
    "    show_action_values(env)\n",
    "\n",
    "    # 获取环境状态\n",
    "    state = state_from_environment(env)\n",
    "    \n",
    "    # 从DQN获取Policy并选取具有最大Value值的动作作为下一个动作\n",
    "    action_id = dqn.next_action(state)\n",
    "    action = env.action_space.action_from_id(action_id)\n",
    "\n",
    "    # debug\n",
    "    #print(\"action:\", action)\n",
    "    \n",
    "    # agent执行此动作\n",
    "    reward, next_location, end = env.step(action)\n",
    "    time.sleep(0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
